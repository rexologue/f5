# runtime/setup/convert_checkpoint.py (–ò–°–ü–†–ê–í–õ–ï–ù–û –¥–ª—è vocab_size)
import os
import re
import json
import time
import argparse
from pathlib import Path
from typing import Any, Dict, Tuple

import torch
import safetensors.torch
from tensorrt_llm.mapping import Mapping
from tensorrt_llm import str_dtype_to_torch

from utils import preload_libpython
preload_libpython()


def split_q_tp(v, tensor_parallel, rank):
    from tensorrt_llm.models.convert_utils import split
    return split(v, tensor_parallel, rank, dim=1).contiguous()


def split_q_bias_tp(v, tensor_parallel, rank):
    from tensorrt_llm.models.convert_utils import split
    return split(v, tensor_parallel, rank, dim=0).contiguous()


def split_matrix_tp(v, tensor_parallel, rank, dim):
    from tensorrt_llm.models.convert_utils import split_matrix_tp as _split
    return _split(v, tensor_parallel, rank, dim=dim)


FACEBOOK_DIT_NAME_MAPPING = {
    r"^transformer_blocks\.(\d+)\.attn\.to_q\.weight$": r"transformer_blocks.\1.attn.to_q.weight",
    r"^transformer_blocks\.(\d+)\.attn\.to_q\.bias$": r"transformer_blocks.\1.attn.to_q.bias",
    r"^transformer_blocks\.(\d+)\.attn\.to_k\.weight$": r"transformer_blocks.\1.attn.to_k.weight",
    r"^transformer_blocks\.(\d+)\.attn\.to_k\.bias$": r"transformer_blocks.\1.attn.to_k.bias",
    r"^transformer_blocks\.(\d+)\.attn\.to_v\.weight$": r"transformer_blocks.\1.attn.to_v.weight",
    r"^transformer_blocks\.(\d+)\.attn\.to_v\.bias$": r"transformer_blocks.\1.attn.to_v.bias",
    r"^transformer_blocks\.(\d+)\.attn\.to_out\.0\.weight$": r"transformer_blocks.\1.attn.to_out.weight",
    r"^transformer_blocks\.(\d+)\.attn\.to_out\.0\.bias$": r"transformer_blocks.\1.attn.to_out.bias",
    r"^transformer_blocks\.(\d+)\.ff\.ff\.0\.0\.weight$": r"transformer_blocks.\1.ff.project_in.weight",
    r"^transformer_blocks\.(\d+)\.ff\.ff\.0\.0\.bias$": r"transformer_blocks.\1.ff.project_in.bias",
    r"^transformer_blocks\.(\d+)\.ff\.ff\.2\.weight$": r"transformer_blocks.\1.ff.ff.weight",
    r"^transformer_blocks\.(\d+)\.ff\.ff\.2\.bias$": r"transformer_blocks.\1.ff.ff.bias",
    r"^transformer_blocks\.(\d+)\.attn_norm\.linear\.weight$": r"transformer_blocks.\1.attn_norm.linear.weight",
    r"^transformer_blocks\.(\d+)\.attn_norm\.linear\.bias$": r"transformer_blocks.\1.attn_norm.linear.bias",
}


def _infer_vocab_size(state_dict: Dict[str, torch.Tensor]) -> tuple[int | None, str | None]:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º vocab_size –∏–∑ text_embed –≤–µ—Å–æ–≤."""
    candidates = [
        "ema_model.text_embed.text_embed.weight",
        "ema_model.transformer.text_embed.text_embed.weight",
        "text_embed.text_embed.weight",
    ]
    for key in candidates:
        if key in state_dict:
            weight = state_dict[key]
            if isinstance(weight, torch.Tensor) and weight.ndim >= 1:
                # –í–ê–ñ–ù–û: –≤—ã—á–∏—Ç–∞–µ–º 1, —Ç.–∫. –≤ –∫–æ–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è +1 –¥–ª—è filler token
                vocab_size = int(weight.shape[0]) - 1
                return vocab_size, key
    return None, None


def convert_timm_dit(args, mapping, dtype="float16") -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
    torch_dtype = str_dtype_to_torch(dtype)
    tensor_parallel = mapping.tp_size
    
    ckpt = torch.load(args.timm_ckpt, map_location="cpu")
    state_dict = ckpt.get("ema_model_state_dict", ckpt)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º vocab_size
    vocab_size, vocab_source = _infer_vocab_size(state_dict)
    
    # Fallback –Ω–∞ –∞—Ä–≥—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if vocab_size is None and args.vocab_size is not None and args.vocab_size > 0:
        vocab_size = args.vocab_size
        vocab_source = "cli_argument"
    
    # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë None - —Å—Ç–∞–≤–∏–º –¥–µ—Ñ–æ–ª—Ç
    if vocab_size is None:
        print("‚ö†Ô∏è  WARNING: vocab_size –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç 2546")
        vocab_size = 2546
        vocab_source = "default"
    
    metadata = {
        "vocab_size": vocab_size,
        "vocab_source": vocab_source,
    }
    
    print(f"üìä Vocab size: {vocab_size} (–∏—Å—Ç–æ—á–Ω–∏–∫: {vocab_source})")
    
    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ transformer –ø–æ–¥–¥–µ—Ä–µ–≤–æ
    prefix = "ema_model.transformer."
    model_params = {
        k[len(prefix):] if k.startswith(prefix) else k: v
        for k, v in state_dict.items()
        if k.startswith("ema_model.transformer")
    }
    
    # –ú–∞–ø–ø–∏–Ω–≥ –∏–º—ë–Ω
    def map_name(src: str) -> str | None:
        for pat, repl in FACEBOOK_DIT_NAME_MAPPING.items():
            if re.match(pat, src):
                return re.sub(pat, repl, src)
        return None
    
    weights = {}
    for name, param in model_params.items():
        new_name = map_name(name)
        if new_name is None:
            continue
        weights[new_name] = param.contiguous().to(torch_dtype)
    
    # TP-—à–∞—Ä–¥–∏–Ω–≥
    for k in list(weights.keys()):
        v = weights[k]
        
        if re.match(r"^transformer_blocks\.\d+\.attn\.to_[qkv]\.weight$", k):
            weights[k] = split_q_tp(v, tensor_parallel, mapping.tp_rank)
        elif re.match(r"^transformer_blocks\.\d+\.attn\.to_[qkv]\.bias$", k):
            weights[k] = split_q_bias_tp(v, tensor_parallel, mapping.tp_rank)
        elif re.match(r"^transformer_blocks\.\d+\.attn\.to_out\.weight$", k):
            weights[k] = split_matrix_tp(v, tensor_parallel, mapping.tp_rank, dim=1)
        elif re.match(r"^transformer_blocks\.\d+\.ff\.project_in\.weight$", k):
            weights[k] = split_matrix_tp(v, tensor_parallel, mapping.tp_rank, dim=1)
        elif re.match(r"^transformer_blocks\.\d+\.ff\.project_in\.bias$", k):
            from tensorrt_llm.models.convert_utils import split
            weights[k] = split(v, tensor_parallel, mapping.tp_rank, dim=0).contiguous()
        elif re.match(r"^transformer_blocks\.\d+\.ff\.ff\.weight$", k):
            weights[k] = split_matrix_tp(v, tensor_parallel, mapping.tp_rank, dim=0)
    
    print(f"‚úÖ Loaded {len(weights)} DiT block weights (TP={tensor_parallel}, rank={mapping.tp_rank})")
    return weights, metadata


def save_config(args, metadata: Dict[str, Any]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º config.json —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú vocab_size."""
    os.makedirs(args.output_dir, exist_ok=True)
    
    vocab_size = metadata.get("vocab_size")
    
    # –í–ê–ñ–ù–û: vocab_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–æ –í–°–ï–• –Ω—É–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
    config = {
        "architecture": "DiT",
        "dtype": args.dtype,
        "hidden_size": args.hidden_size,
        "num_hidden_layers": args.depth,
        "num_attention_heads": args.num_heads,
        "dim_head": args.hidden_size // args.num_heads,
        "vocab_size": vocab_size,  # <- –¥–æ–±–∞–≤–ª–µ–Ω–æ –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
        "mapping": {
            "world_size": args.tp_size * args.pp_size,
            "tp_size": args.tp_size,
            "pp_size": args.pp_size,
        },
        "builder_config": {
            "precision": args.dtype,
            "vocab_size": vocab_size,  # <- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∑–¥–µ—Å—å
        },
        "pretrained_config": {
            "vocab_size": vocab_size,  # <- –∏ –∑–¥–µ—Å—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        },
    }
    
    if metadata.get("vocab_source"):
        config["metadata"] = {"vocab_source": metadata["vocab_source"]}
    
    config_path = os.path.join(args.output_dir, "config.json")
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    print(f"‚úÖ Config —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {config_path}")
    print(f"   vocab_size: {vocab_size}")


def convert_and_save(args, rank):
    mapping = Mapping(
        world_size=args.tp_size * args.pp_size,
        rank=rank,
        tp_size=args.tp_size,
        pp_size=args.pp_size,
    )
    
    weights, metadata = convert_timm_dit(args, mapping, dtype=args.dtype)
    
    if rank == 0:
        save_config(args, metadata)
    
    safetensors.torch.save_file(
        weights,
        os.path.join(args.output_dir, f"rank{rank}.safetensors")
    )


def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument("--timm_ckpt", type=str, required=True)
    parser.add_argument("--output_dir", type=str, default="./tllm_checkpoint_dit")
    parser.add_argument("--hidden_size", type=int, default=1024)
    parser.add_argument("--depth", type=int, default=22)
    parser.add_argument("--num_heads", type=int, default=16)
    parser.add_argument("--tp_size", type=int, default=1)
    parser.add_argument("--pp_size", type=int, default=1)
    parser.add_argument("--dtype", type=str, default="float16", choices=["float32", "bfloat16", "float16"])
    parser.add_argument("--workers", type=int, default=1)
    parser.add_argument("--vocab-size", type=int, default=None, help="Override vocab size")
    return parser.parse_args()


def main():
    args = parse_arguments()
    assert args.pp_size == 1, "PP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
    
    world_size = args.tp_size * args.pp_size
    
    print(f"üöÄ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DiT checkpoint ‚Üí TRT-LLM (TP={args.tp_size})")
    
    for rank in range(world_size):
        convert_and_save(args, rank)
    
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.output_dir}")


if __name__ == "__main__":
    main()